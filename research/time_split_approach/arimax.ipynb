{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76f86e7-d996-4c31-b79d-456aa4fae6c8",
   "metadata": {},
   "source": [
    "# ARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc8663-39a3-4b1e-b733-89871008767e",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e484e1e-feeb-4456-bfef-fc8baebdf853",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smts\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose, STL\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "from typing import Union\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7b25750a-1130-4915-918a-11a66e72482b",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "def plot_stl_decomposition(data, period=7, color='#636EFA', title=''):\n",
    "    # STL Decomposition\n",
    "    decomposition = STL(data, period=period).fit()\n",
    "    res_season = decomposition.seasonal.dropna()\n",
    "    lbvalue, pvalue = acorr_ljungbox(res_season, lags=3)\n",
    "    print(f'Ljung-Box Test p-value: {pvalue[0]}')\n",
    "\n",
    "    # Plotting\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=4, ncols=1, sharex=True, dpi=300, figsize=(12,4))\n",
    "    fig.suptitle(title, y=1.02)\n",
    "    ax1.plot(decomposition.observed, color=color)\n",
    "    ax1.set_ylabel('Observed')\n",
    "    ax2.plot(decomposition.trend, color='#00CC96')\n",
    "    ax2.set_ylabel('Trend')\n",
    "    ax3.plot(decomposition.seasonal, color='#FECB52')\n",
    "    ax3.set_ylabel('Seasonal')\n",
    "    ax4.plot(decomposition.resid, color='#FFA15A')\n",
    "    ax4.set_ylabel('Residuals')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_aic(y_true, y_pred, num_features):\n",
    "    resid = y_true - y_pred\n",
    "    sse = np.sum(resid ** 2)\n",
    "    aic = len(y_true) * np.log(sse / len(y_true)) + 2 * num_features\n",
    "    return aic\n",
    "\n",
    "\n",
    "def calculate_bic(y_true, y_pred, num_features):\n",
    "    resid = y_true - y_pred\n",
    "    sse = np.sum(resid ** 2)\n",
    "    n = len(y_true)\n",
    "    bic = n * np.log(sse / n) + num_features * np.log(n)\n",
    "    return bic\n",
    "\n",
    "\n",
    "def tsplot(y, lags=None, figsize=(20, 10), title=None):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    layout = (2, 2)\n",
    "    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "    acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "    pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "\n",
    "    y.plot(ax=ts_ax)\n",
    "    p_value = smts.adfuller(y)[1]\n",
    "    if title is None:\n",
    "        title = 'Time Series Analysis Plots'\n",
    "    ts_ax.set_title('{0}\\n Dickey-Fuller: p={1:.5f}'.format(title, p_value))\n",
    "    \n",
    "    smts.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "    smts.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def forward_step(X_train, y_train, selected_features, remaining_features, criterion_func):\n",
    "    best_criterion = np.inf\n",
    "    best_feature = None\n",
    "    \n",
    "    for feature in remaining_features:\n",
    "        temp_features = selected_features + [feature]\n",
    "        temp_model = sm.OLS(y_train, sm.add_constant(X_train[temp_features])).fit()\n",
    "        temp_criterion = criterion_func(y_train, temp_model.predict(sm.add_constant(X_train[temp_features])), len(temp_model.params))\n",
    "        \n",
    "        if temp_criterion < best_criterion:\n",
    "            best_criterion = temp_criterion\n",
    "            best_feature = feature\n",
    "    \n",
    "    return best_criterion, best_feature\n",
    "\n",
    "def backward_step(X_train, y_train, selected_features, criterion_func):\n",
    "    best_criterion = np.inf\n",
    "    best_feature = None\n",
    "    \n",
    "    for feature in selected_features:\n",
    "        temp_features = selected_features.copy()\n",
    "        temp_features.remove(feature)\n",
    "        temp_model = sm.OLS(y_train, sm.add_constant(X_train[temp_features])).fit()\n",
    "        temp_criterion = criterion_func(y_train, temp_model.predict(sm.add_constant(X_train[temp_features])), len(temp_model.params))\n",
    "        \n",
    "        if temp_criterion < best_criterion:\n",
    "            best_criterion = temp_criterion\n",
    "            best_feature = feature\n",
    "    \n",
    "    return best_criterion, best_feature\n",
    "\n",
    "def stepwise_bidirectional_selection(X_train, y_train, method='aic'):\n",
    "    features = list(X_train.columns)\n",
    "    selected_features = []\n",
    "    best_criterion = np.inf\n",
    "    \n",
    "    while len(features) > 0:\n",
    "        if method == 'aic':\n",
    "            forward_criterion, forward_feature = forward_step(X_train, y_train, selected_features, features, calculate_aic)\n",
    "            backward_criterion, backward_feature = backward_step(X_train, y_train, selected_features, calculate_aic)\n",
    "        elif method == 'bic':\n",
    "            forward_criterion, forward_feature = forward_step(X_train, y_train, selected_features, features, calculate_bic)\n",
    "            backward_criterion, backward_feature = backward_step(X_train, y_train, selected_features, calculate_bic)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid criterion_func. Use 'aic' or 'bic'.\")\n",
    "        \n",
    "        if forward_criterion < backward_criterion and forward_criterion < best_criterion:\n",
    "            selected_features.append(forward_feature)\n",
    "            best_criterion = forward_criterion\n",
    "        elif backward_criterion < forward_criterion and backward_criterion < best_criterion:\n",
    "            selected_features.remove(backward_feature)\n",
    "            best_criterion = backward_criterion\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "\n",
    "def fit_arima_model(order, endog, d):\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            model = SARIMAX(endog, order=(order[0], d, order[1]), simple_differencing=False).fit(disp=False)\n",
    "            aic = model.aic\n",
    "            bic = model.bic\n",
    "            return [(order[0], d, order[1]), round(aic, 2), round(bic, 2)]\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def optimize_ARIMA(endog: Union[pd.Series, list], order_list: list, d: int) -> pd.DataFrame:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        results = Parallel(n_jobs=-1)(delayed(fit_arima_model)(order, endog, d) for order in tqdm(order_list))\n",
    "    \n",
    "    results = [result for result in results if result is not None]\n",
    "\n",
    "    result_df = pd.DataFrame(results, columns=['(p, d, q)', 'AIC', 'BIC'])\n",
    "    result_df = result_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\n",
    "    order_arima = result_df['(p, d, q)'].iloc[0]    \n",
    "    \n",
    "    return result_df, order_arima\n",
    "\n",
    "\n",
    "def fit_arimax_model(order, endog, exog, d):\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            model = SARIMAX(endog, exog=exog, order=(order[0], d, order[1]), simple_differencing=False).fit(disp=False)\n",
    "            aic = model.aic\n",
    "            bic = model.bic\n",
    "            return [(order[0], d, order[1]), round(aic, 2), round(bic, 2)]\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def optimize_ARIMAX(endog: Union[pd.Series, list], exog: Union[pd.Series, list], order_list: list, d: int) -> pd.DataFrame:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        results = Parallel(n_jobs=-1)(delayed(fit_arimax_model)(order, endog, exog, d) for order in tqdm(order_list))\n",
    "    \n",
    "    results = [result for result in results if result is not None]\n",
    "\n",
    "    result_df = pd.DataFrame(results, columns=['(p, d, q)', 'AIC', 'BIC'])\n",
    "    result_df = result_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\n",
    "    order_arimax = result_df['(p, d, q)'].iloc[0]    \n",
    "    \n",
    "    return result_df, order_arimax\n",
    "\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    non_zero_indices = np.where(y_true != 0)[0]\n",
    "    y_true_no_zeros = np.array(y_true)[non_zero_indices]\n",
    "    y_pred_no_zeros = np.array(y_pred)[non_zero_indices]\n",
    "\n",
    "    absolute_percentage_errors = np.abs((y_true_no_zeros - y_pred_no_zeros) / y_true_no_zeros)\n",
    "    mape_value = np.mean(absolute_percentage_errors) * 100\n",
    "    return mape_value\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d8064a7f-edf1-4922-9408-f556155eac46",
   "metadata": {},
   "source": [
    "df = pd.read_csv('data_sensors_rovere.csv')\n",
    "df = df.rename(columns={'group': 'group_id'})\n",
    "\n",
    "df_rovere = df[['reading_id', 'timestamp', 'sensor_id', 'value', 'description', 'group_id']]\n",
    "\n",
    "df_rovere['reading_id'] = df_rovere['reading_id'].astype(str)\n",
    "df_rovere['timestamp'] = pd.to_datetime(df_rovere['timestamp']).dt.floor('D').dt.date\n",
    "df_rovere['sensor_id'] = df_rovere['sensor_id'].astype(str)\n",
    "df_rovere['value'] = df_rovere['value'].astype(float)\n",
    "df_rovere['description'] = df_rovere['description'].astype(str)\n",
    "df_rovere['group_id'] = df_rovere['group_id'].astype(str)\n",
    "\n",
    "condition_30 = df_rovere['sensor_id'].isin(['72', '76', '73', '74', '61', '63', '67', '65'])\n",
    "condition_60 = df_rovere['sensor_id'].isin(['71', '69', '75', '70', '62', '64', '68', '66'])\n",
    "condition_irrigation = df_rovere['description'] == 'irrigation'\n",
    "\n",
    "df_rovere.loc[condition_30, 'description'] = 'Tensiometer 30'\n",
    "df_rovere.loc[condition_60, 'description'] = 'Tensiometer 60'\n",
    "df_rovere.loc[condition_irrigation, 'description'] = 'Irrigation'\n",
    "\n",
    "print('Shape:', df_rovere.shape)\n",
    "print('Types:\\n', df_rovere.dtypes)\n",
    "df_rovere.head(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "35bcb7db-d7b1-403a-a884-2d9151eeffb3",
   "metadata": {},
   "source": [
    "sensor_group = df_rovere[['sensor_id', 'description', 'group_id']].drop_duplicates().sort_values(by='group_id').reset_index(drop=True)\n",
    "sensor_group"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2d06b5d9-9b9c-4217-9872-abcc5c1f460b",
   "metadata": {},
   "source": [
    "df_group_1 = df_rovere[df_rovere['group_id'] == '1'].reset_index(drop=True)\n",
    "df_group_1 = df_group_1.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_1.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_1 = df_group_1.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_1.columns.name = None\n",
    "df_pivot_1.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "\n",
    "df_pivot_1 = df_pivot_1.dropna().reset_index(drop=True)\n",
    "df = df_pivot_1\n",
    "\n",
    "\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dcd96a6a-272f-46d7-987c-74098f1c0008",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "43264066-ccb5-424c-a7dc-6ddadbcf67ff",
   "metadata": {},
   "source": [
    "round(df.describe(),2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1d849299-6ccf-4483-b3cf-18eae14a3151",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ce7b5482-0c32-4dfb-b3bf-dec69bbe2677",
   "metadata": {},
   "source": [
    "# Distributions of Tensiometers\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Distribution of Tensiometer (Depth=30)\n",
    "\n",
    "# Boxplot\n",
    "sns.boxplot(y=df['avg_tens30'], ax=axes[0, 0], color='#636EFA', width=0.5)\n",
    "axes[0, 0].set_title('Boxplot')\n",
    "axes[0, 0].set_ylabel('Tensiometer (30)')\n",
    "\n",
    "# Histogram with KDE\n",
    "axes[0, 1].hist(df['avg_tens30'], bins=15, color='#636EFA', edgecolor='black', density=True)\n",
    "sns.kdeplot(df['avg_tens30'], color='#FFA15A', ax=axes[0, 1])\n",
    "mu, sigma = df['avg_tens30'].mean(), df['avg_tens30'].std()\n",
    "xmin, xmax = axes[0, 1].set_xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = stats.norm.pdf(x, mu, sigma)\n",
    "axes[0, 1].plot(x, p, '#00CC96', linewidth=2)\n",
    "\n",
    "axes[0, 1].set_title('Histogram with KDE')\n",
    "axes[0, 1].set_xlabel('Tensiometer (30)')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "\n",
    "# Q-Q Plot\n",
    "stats.probplot(df['avg_tens30'], dist='norm', plot=axes[0, 2])\n",
    "axes[0, 2].set_title('Gaussian Q-Q Plot')\n",
    "axes[0, 2].set_xlabel('Theoretical Quantile')\n",
    "axes[0, 2].set_ylabel('Observed Quantile')\n",
    "\n",
    "\n",
    "# Distribution of Tensiometer (Depth=60)\n",
    "sns.boxplot(y=df['avg_tens60'], ax=axes[1, 0], color='#EF553B', width=0.5)\n",
    "axes[1, 0].set_title('Boxplot')\n",
    "axes[1, 0].set_ylabel('Tensiometer (60)')\n",
    "\n",
    "axes[1, 1].hist(df['avg_tens60'], bins=15, color='#EF553B', edgecolor='black', density=True)\n",
    "sns.kdeplot(df['avg_tens60'], color='#FFA15A', ax=axes[1, 1])\n",
    "mu, sigma = df['avg_tens60'].mean(), df['avg_tens60'].std()\n",
    "xmin, xmax = axes[1, 1].set_xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = stats.norm.pdf(x, mu, sigma)\n",
    "axes[1, 1].plot(x, p, '#00CC96', linewidth=2)\n",
    "\n",
    "axes[1, 1].set_title('Histogram with KDE')\n",
    "axes[1, 1].set_xlabel('Tensiometer (60)')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "\n",
    "stats.probplot(df['avg_tens30'], dist='norm', plot=axes[1, 2])\n",
    "axes[1, 2].set_title('Gaussian Q-Q Plot')\n",
    "axes[1, 2].set_xlabel('Theoretical Quantile')\n",
    "axes[1, 2].set_ylabel('Observed Quantile')\n",
    "\n",
    "plt.suptitle('Tensiometer Distribution at Depths 30 and 60', fontsize=18)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97]) \n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d4b04b9d-62bb-4156-9976-31430aebba66",
   "metadata": {},
   "source": [
    "# Distributions of the main Covariates\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "variables = ['avg_hum', 'avg_temp', 'avg_solar', 'sum_irr', 'sum_rain']\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    sns.histplot(df[var], bins=15, color='#00CC96', edgecolor='black', kde=True, ax=axes[0, i])\n",
    "    sns.boxplot(y=df[var], ax=axes[1, i], color='#00CC96', width=0.5)\n",
    "    \n",
    "    axes[0, i].set_title(f'{var}')\n",
    "    axes[0, i].set_xlabel(var)\n",
    "    axes[0, i].set_ylabel('Density')\n",
    "    axes[1, i].set_ylabel(var)\n",
    "\n",
    "plt.suptitle('Distributions of Covariates', fontsize=18)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c113c146-c23b-4c22-850d-a97fcd4c81a7",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "74ecac5f-eea1-4903-98b6-c0ac0646e262",
   "metadata": {},
   "source": [
    "# Scatterplots between Target Variables and the main Covariates\n",
    "\n",
    "selected_columns = df[['avg_tens30', 'avg_tens60', 'avg_hum', 'avg_temp', 'avg_solar', 'sum_irr', 'sum_rain']]\n",
    "\n",
    "sns.pairplot(selected_columns, kind='reg', height=1.5, aspect=2, plot_kws={'line_kws':{'color':'red'}})\n",
    "plt.suptitle('Scatterplots', y=1.02, fontsize=18)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "03f48d5c-ee32-4e11-99dd-d3a60b75ca3d",
   "metadata": {},
   "source": [
    "# Heatmap between Target Variables and the main Covariates\n",
    "\n",
    "corr_mat = selected_columns.corr()\n",
    "corr_mat.sort_values(by='avg_tens30', axis=0, ascending=False, inplace=True)\n",
    "corr_mat.sort_values(by='avg_tens30', axis=1, ascending=False, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_mat, annot=True, cmap='coolwarm', vmin=-1, vmax=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "964ca411-cb52-4d64-a8b6-df72dd68753c",
   "metadata": {},
   "source": [
    "# Heatmap between Target Variables all the possible Covariates\n",
    "\n",
    "corr_mat_all = round(df.iloc[:, 1:].corr(), 2)\n",
    "corr_mat_all.sort_values(by='avg_tens30', axis=0, ascending=False, inplace=True)\n",
    "corr_mat_all.sort_values(by='avg_tens30', axis=1, ascending=False, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(corr_mat_all, annot=True, cmap='coolwarm', vmin=-1, vmax=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "268fa634-c21d-4013-83e2-0213951184c6",
   "metadata": {},
   "source": [
    "### Time Series EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f70418c0-e7e7-443e-ae68-77d3f8664652",
   "metadata": {},
   "source": [
    "# Time Series of the Tensiometers\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.plot(df['date'], df['avg_tens30'], linestyle='-', label='Avg Tensiometer (30)', color='#636EFA')\n",
    "plt.plot(df['date'], df['avg_tens60'], linestyle='-', label='Avg Tensiometer (60)', color='#EF553B')\n",
    "\n",
    "plt.title('Tensiometer Comparison (30 vs 60)')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "locator = mdates.MonthLocator(bymonthday=1)\n",
    "formatter = mdates.DateFormatter('%b')\n",
    "\n",
    "plt.gca().xaxis.set_major_locator(locator)\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3808837f-c7ca-47ae-bce6-1eaa6fc03860",
   "metadata": {},
   "source": [
    "# Seasonal and Trend Decomposition using Loess (STL)\n",
    "\n",
    "## Weekly Decomporition\n",
    "\n",
    "plot_stl_decomposition(df['avg_tens30'], period=7, title='STL Decomposition - Tensiometer (30)')\n",
    "plot_stl_decomposition(df['avg_tens60'], period=7, color='#EF553B', title='STL Decomposition - Tensiometer (60)')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fc3784bf-d6c7-4759-98fe-d21281d2e3b3",
   "metadata": {},
   "source": [
    "## Monthly Decomposition\n",
    "\n",
    "plot_stl_decomposition(df['avg_tens30'], period=30, title='STL Decomposition - Tensiometer (30)')\n",
    "plot_stl_decomposition(df['avg_tens60'], period=30, color='#EF553B', title='STL Decomposition - Tensiometer (60)')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ab7307d7-4710-4788-881c-fb88dac877de",
   "metadata": {},
   "source": [
    "# Time Series of the Main Features\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, dpi=300, figsize=(12, 6))\n",
    "\n",
    "colors = ['#636EFA', '#EF553B', '#00CC96', '#00CC96', '#00CC96', '#00CC96', '#00CC96']\n",
    "variables = ['avg_tens30', 'avg_tens60', 'sum_irr', 'avg_hum', 'sum_rain', 'avg_temp', 'avg_solar']\n",
    "\n",
    "for i, (variable, ax) in enumerate(zip(variables, axes.flatten())):\n",
    "    data = df[variable]\n",
    "    ax.plot(data, color=colors[i], linewidth=1)\n",
    "    ax.set_title(variable)\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines['top'].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "for j in range(i + 1, len(axes.flat)):\n",
    "    axes.flatten()[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "02b2e874-4fc7-4eec-821d-8308e301b6e0",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7a52b-6de5-469e-a295-62feccda65b4",
   "metadata": {},
   "source": [
    "### Group 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "58e3fe01-f819-47bf-999b-8271c11f3feb",
   "metadata": {},
   "source": [
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens30']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b1524e89-c4b3-4b49-ae22-85926063d1d8",
   "metadata": {},
   "source": [
    "# Analysis of Causality with Granger Causality Test\n",
    "\n",
    "## Analysis of Causes on the Tensiometer (30) based on 1 past day\n",
    "\n",
    "print('Tensiometer (60)')\n",
    "granger_1 = grangercausalitytests(df[['avg_tens30', 'avg_tens60']], [1])\n",
    "\n",
    "print('\\nHumidity')\n",
    "granger_2 = grangercausalitytests(df[['avg_tens30', 'avg_hum']], [1])\n",
    "\n",
    "print('\\nTemperature')\n",
    "granger_3 = grangercausalitytests(df[['avg_tens30', 'avg_temp']], [1])\n",
    "\n",
    "print('\\nSolar Radiation')\n",
    "granger_4 = grangercausalitytests(df[['avg_tens30', 'avg_solar']], [1])\n",
    "\n",
    "print('\\nIrrigation')\n",
    "granger_5 = grangercausalitytests(df[['avg_tens30', 'sum_irr']], [1])\n",
    "\n",
    "print('\\nRain')\n",
    "granger_6 = grangercausalitytests(df[['avg_tens30', 'sum_rain']], [1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "335b3860-05b2-495f-97e8-8802f3ae255b",
   "metadata": {},
   "source": [
    "## Analysis of Causes on the Tensiometer (30) based on 3 past days\n",
    "\n",
    "print('Tensiometer (60)')\n",
    "granger_1 = grangercausalitytests(df[['avg_tens30', 'avg_tens60']], [3])\n",
    "\n",
    "print('\\nHumidity')\n",
    "granger_2 = grangercausalitytests(df[['avg_tens30', 'avg_hum']], [3])\n",
    "\n",
    "print('\\nTemperature')\n",
    "granger_3 = grangercausalitytests(df[['avg_tens30', 'avg_temp']], [3])\n",
    "\n",
    "print('\\nSolar Radiation')\n",
    "granger_4 = grangercausalitytests(df[['avg_tens30', 'avg_solar']], [3])\n",
    "\n",
    "print('\\nIrrigation')\n",
    "granger_5 = grangercausalitytests(df[['avg_tens30', 'sum_irr']], [3])\n",
    "\n",
    "print('\\nRain')\n",
    "granger_6 = grangercausalitytests(df[['avg_tens30', 'sum_rain']], [3])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0aae0208-26ff-431c-ac0e-5232a28b6d6e",
   "metadata": {},
   "source": [
    "## Analysis of Causes on the Tensiometer (30) based on 5 past days\n",
    "\n",
    "print('Tensiometer (60)')\n",
    "granger_1 = grangercausalitytests(df[['avg_tens30', 'avg_tens60']], [5])\n",
    "\n",
    "print('\\nHumidity')\n",
    "granger_2 = grangercausalitytests(df[['avg_tens30', 'avg_hum']], [5])\n",
    "\n",
    "print('\\nTemperature')\n",
    "granger_3 = grangercausalitytests(df[['avg_tens30', 'avg_temp']], [5])\n",
    "\n",
    "print('\\nSolar Radiation')\n",
    "granger_4 = grangercausalitytests(df[['avg_tens30', 'avg_solar']], [5])\n",
    "\n",
    "print('\\nIrrigation')\n",
    "granger_5 = grangercausalitytests(df[['avg_tens30', 'sum_irr']], [5])\n",
    "\n",
    "print('\\nRain')\n",
    "granger_6 = grangercausalitytests(df[['avg_tens30', 'sum_rain']], [5])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b05e3e05-715b-4650-8720-8cfec4253eab",
   "metadata": {},
   "source": [
    "## Analysis of Causes on the Tensiometer (60) based on 1 past day\n",
    "\n",
    "print('Tensiometer (30)')\n",
    "granger_1 = grangercausalitytests(df[['avg_tens60', 'avg_tens30']], [1])\n",
    "\n",
    "print('\\nHumidity')\n",
    "granger_2 = grangercausalitytests(df[['avg_tens60', 'avg_hum']], [1])\n",
    "\n",
    "print('\\nTemperature')\n",
    "granger_3 = grangercausalitytests(df[['avg_tens60', 'avg_temp']], [1])\n",
    "\n",
    "print('\\nSolar Radiation')\n",
    "granger_4 = grangercausalitytests(df[['avg_tens60', 'avg_solar']], [1])\n",
    "\n",
    "print('\\nIrrigation')\n",
    "granger_5 = grangercausalitytests(df[['avg_tens60', 'sum_irr']], [1])\n",
    "\n",
    "print('\\nRain')\n",
    "granger_6 = grangercausalitytests(df[['avg_tens60', 'sum_rain']], [1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "634ef892-70e4-429c-be54-cde505a34617",
   "metadata": {},
   "source": [
    "## Analysis of Causes on the Tensiometer (60) based on 3 past days\n",
    "\n",
    "print('Tensiometer (30)')\n",
    "granger_1 = grangercausalitytests(df[['avg_tens60', 'avg_tens30']], [3])\n",
    "\n",
    "print('\\nHumidity')\n",
    "granger_2 = grangercausalitytests(df[['avg_tens60', 'avg_hum']], [3])\n",
    "\n",
    "print('\\nTemperature')\n",
    "granger_3 = grangercausalitytests(df[['avg_tens60', 'avg_temp']], [3])\n",
    "\n",
    "print('\\nSolar Radiation')\n",
    "granger_4 = grangercausalitytests(df[['avg_tens60', 'avg_solar']], [3])\n",
    "\n",
    "print('\\nIrrigation')\n",
    "granger_5 = grangercausalitytests(df[['avg_tens60', 'sum_irr']], [3])\n",
    "\n",
    "print('\\nRain')\n",
    "granger_6 = grangercausalitytests(df[['avg_tens60', 'sum_rain']], [3])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9c3aa3d6-4d2a-469f-8a94-d2c278bc28e5",
   "metadata": {},
   "source": [
    "## Analysis of Causes on the Tensiometer (60) based on 5 past days\n",
    "\n",
    "print('Tensiometer (30)')\n",
    "granger_1 = grangercausalitytests(df[['avg_tens60', 'avg_tens30']], [5])\n",
    "\n",
    "print('\\nHumidity')\n",
    "granger_2 = grangercausalitytests(df[['avg_tens60', 'avg_hum']], [5])\n",
    "\n",
    "print('\\nTemperature')\n",
    "granger_3 = grangercausalitytests(df[['avg_tens60', 'avg_temp']], [5])\n",
    "\n",
    "print('\\nSolar Radiation')\n",
    "granger_4 = grangercausalitytests(df[['avg_tens60', 'avg_solar']], [5])\n",
    "\n",
    "print('\\nIrrigation')\n",
    "granger_5 = grangercausalitytests(df[['avg_tens60', 'sum_irr']], [5])\n",
    "\n",
    "print('\\nRain')\n",
    "granger_6 = grangercausalitytests(df[['avg_tens60', 'sum_rain']], [5])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "198ec32a-61ea-4e81-bc60-d68df515bd42",
   "metadata": {},
   "source": [
    "#### Sensor 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f5cf76bb-0f95-44e1-a5b2-7c4d32afc94a",
   "metadata": {},
   "source": [
    "# Linear Regression and Feature Selection\n",
    "\n",
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "# Predictions with Naive Method on the entire dataset\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2ac7900a-728d-45a5-8055-4be90d79191c",
   "metadata": {},
   "source": [
    "# Train-Test Split\n",
    "\n",
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ff822ce-39fc-478a-b69f-25d11d682e29",
   "metadata": {},
   "source": [
    "# Hyperparameters Tuning\n",
    "\n",
    "## Stationary Verification (choice of the integrated terms d)\n",
    "\n",
    "tsplot(y_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "da3fd08d-8772-4d84-8b00-061923f0a258",
   "metadata": {},
   "source": [
    "y_train_diff = y_train.diff().dropna()\n",
    "tsplot(y_train_diff)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2d630a3f-6e20-424a-a7ca-3ee0bb91dcca",
   "metadata": {},
   "source": [
    "## AIC search (choice of the number of the autoregressive terms p and the number of moving average terms q) for ARIMA model\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ff8d012a-bed7-4a5b-ac90-c6bb672840ca",
   "metadata": {},
   "source": [
    "## AIC search (choice of the number of the autoregressive terms p and the number of moving average terms q) for ARIMAX model\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "result_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "46d137b2-a396-47da-930a-4dc1e6aacdc0",
   "metadata": {},
   "source": [
    "# Model Training and Coefficients Analysis\n",
    "\n",
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "print(results.summary())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "28519b5f-ddd4-42f5-b480-1531f5869cca",
   "metadata": {},
   "source": [
    "# Model Diagnostics with the Residuals Analysis\n",
    "\n",
    "results.plot_diagnostics(figsize=(20,10))\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2f27aeb9-fdda-42db-ae1e-2fc39f435e52",
   "metadata": {},
   "source": [
    "# Forecasting on Test Set\n",
    "\n",
    "## ARIMA Rolling Forecasting\n",
    "\n",
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "## ARIMAX Rolling Forecasting\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "df_predictions = pd.DataFrame({'Real': y_test, 'Naive Prediction': naive_pred[-len(y_test):], 'Linear Prediction': linear_pred, 'ARIMA Prediction': arima_pred, 'ARIMAX Prediction': arimax_pred})\n",
    "df_predictions.head(15)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3a5b9c97-5689-4ccf-9b23-faf5f8e9b0c6",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8caeaa1f-20cd-407f-b8c6-a80a07752a5b",
   "metadata": {},
   "source": [
    "# Predicted Time Series Plot\n",
    "\n",
    "residuals_naive = y_test - naive_pred[-len(y_test):]\n",
    "residuals_ARIMAX = y_test - arimax_pred\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "combined_index = np.concatenate([y_train.index, y_test.index])\n",
    "combined_index_1 = np.concatenate([y_train.index, y_test[:-1].index])\n",
    "\n",
    "new_indices = np.arange(len(y_train), len(y_train) + len(y_test))\n",
    "arimax_pred_df = pd.DataFrame(arimax_pred, index=new_indices, columns=['ARIMAX Prediction'])\n",
    "\n",
    "ax.plot(combined_index, np.concatenate([y_train, y_test]), label='Original Series', color='blue')\n",
    "ax.plot(y_test, 'b-', label='Actual', color='black')\n",
    "\n",
    "ax.plot(naive_pred[-len(y_test):], 'r:', label='Naive Method', color='red')\n",
    "ax.scatter(combined_index[-len(y_test):], residuals_naive, color='orange', label='Residuals Naive', marker='o')\n",
    "\n",
    "ax.plot(arimax_pred_df.iloc[:-1], 'k--', label='ARIMAX', color='green')\n",
    "ax.scatter(combined_index[-len(y_test):-1], residuals_ARIMAX[:-1], color='purple', label='Residuals ARIMAX', marker='x')\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Avg Tens 30')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "522e2768-05e6-4794-a6a5-8213ec8cf10d",
   "metadata": {},
   "source": [
    "#### Sensor 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c1b95446-5f3d-4b0d-86ad-9628d2df26d7",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "58860ec8-b1a8-44a9-bc13-7d85a5c1f9a6",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "95d61d5a-cb8e-41a1-96f9-30708b7f1abc",
   "metadata": {},
   "source": [
    "tsplot(y_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d832014c-2930-402d-a4ee-33c88557467e",
   "metadata": {},
   "source": [
    "y_train_diff = y_train.diff().dropna()\n",
    "tsplot(y_train_diff)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1f743e83-1b96-45ec-a79f-527c99b07b77",
   "metadata": {},
   "source": [
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9fa7cf25-b47a-49c7-914a-ca4dfe904d5b",
   "metadata": {},
   "source": [
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "result_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f3c08dd6-e9c1-440b-9ed3-ec50b2a3f875",
   "metadata": {},
   "source": [
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "print(results.summary())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5477423a-29a3-46e1-a54a-337233c2b8d6",
   "metadata": {},
   "source": [
    "results.plot_diagnostics(figsize=(20,10))\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e0eac5ee-e01b-4198-9fcc-4f0e6d9d21df",
   "metadata": {},
   "source": [
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "df_predictions = pd.DataFrame({'Real': y_test, 'Naive Prediction': naive_pred[-len(y_test):], 'Linear Prediction': linear_pred, 'ARIMA Prediction': arima_pred, 'ARIMAX Prediction': arimax_pred})\n",
    "df_predictions.head(15)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "22a7ee11-9f03-48e7-b986-ed6e3c812073",
   "metadata": {},
   "source": [
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "47a849ea-4c4f-4c8a-ac4e-26ddcd6b1166",
   "metadata": {},
   "source": [
    "residuals_naive = y_test - naive_pred[-len(y_test):]\n",
    "residuals_ARIMAX = y_test - arimax_pred\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "combined_index = np.concatenate([y_train.index, y_test.index])\n",
    "combined_index_1 = np.concatenate([y_train.index, y_test[:-1].index])\n",
    "\n",
    "new_indices = np.arange(len(y_train), len(y_train) + len(y_test))\n",
    "arimax_pred_df = pd.DataFrame(arimax_pred, index=new_indices, columns=['ARIMAX Prediction'])\n",
    "\n",
    "ax.plot(combined_index, np.concatenate([y_train, y_test]), label='Train', color='blue')\n",
    "ax.plot(y_test, 'b-', label='Test', color='black')\n",
    "\n",
    "ax.plot(naive_pred[-len(y_test):], 'r:', label='Naive Method', color='red')\n",
    "ax.scatter(combined_index[-len(y_test):], residuals_naive, color='orange', label='Naive Residuals', marker='o')\n",
    "\n",
    "ax.plot(arimax_pred_df.iloc[:-1], 'k--', label='ARIMAX', color='green')\n",
    "ax.scatter(combined_index[-len(y_test):-1], residuals_ARIMAX[:-1], color='purple', label='ARIMAX Residuals', marker='x')\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Avg Tens 60')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "55a2ae57-1e96-44c4-a419-901c4f4e181a",
   "metadata": {},
   "source": [
    "### Group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05e299b9-2da7-4c2f-b2f6-5de558f652f1",
   "metadata": {},
   "source": [
    "df_group_2 = df_rovere[df_rovere['group_id'] == '2'].reset_index(drop=True)\n",
    "df_group_2 = df_group_2.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_2.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_2 = df_group_2.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_2.columns.name = None\n",
    "df_pivot_2.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "df_pivot_2 = df_pivot_2.dropna().reset_index(drop=True)\n",
    "\n",
    "df = df_pivot_2\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens30']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1b65b426-991b-4e8e-b8f1-cddd5fb6a93a",
   "metadata": {},
   "source": [
    "#### Sensor 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f59faff0-2187-4dc4-b7ff-88496a5c0638",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4969d717-a8e3-4d30-b321-1616244812aa",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "\n",
    "\n",
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ed5b30c5-551e-41ef-8518-3357a025095c",
   "metadata": {},
   "source": [
    "#### Sensor 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a5f36541-2565-45b4-b60f-dcc51a7691eb",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "742ebb79-99a4-4a7c-b87f-7cd0ee6d7e46",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "\n",
    "\n",
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "acc06518-a647-49dd-96db-f872508b2dfe",
   "metadata": {},
   "source": [
    "### Group 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a104af35-6f93-46fa-ac60-6bf2de55ef87",
   "metadata": {},
   "source": [
    "df_group_3 = df_rovere[df_rovere['group_id'] == '3'].reset_index(drop=True)\n",
    "df_group_3 = df_group_3.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_3.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_3 = df_group_3.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_3.columns.name = None\n",
    "df_pivot_3.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "df_pivot_3 = df_pivot_3.dropna().reset_index(drop=True)\n",
    "\n",
    "df = df_pivot_3\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens30']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "42de9386-1420-4215-9d3f-cdc988bf9c13",
   "metadata": {},
   "source": [
    "#### Sensor 73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d65ba63-e120-498f-85a0-bba8ec7810ce",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0a199a5-e774-47f4-80ae-b15baea776b5",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "\n",
    "\n",
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "438cb87c-fa1b-4052-9ce5-f6b87a7fedf1",
   "metadata": {},
   "source": [
    "#### Sensor 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a6ba592-1e31-4dec-b480-d3628cb00db3",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "666b7e20-1af3-4748-bcdd-972d3a1c7c34",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "\n",
    "\n",
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e31f471d-3deb-4457-a90e-8dfec2a29da9",
   "metadata": {},
   "source": [
    "### Group 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7dbfa81-3c35-4573-8ad8-c8e75d9c4038",
   "metadata": {},
   "source": [
    "df_group_4 = df_rovere[df_rovere['group_id'] == '4'].reset_index(drop=True)\n",
    "df_group_4 = df_group_4.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_4.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_4 = df_group_4.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_4.columns.name = None\n",
    "df_pivot_4.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "df_pivot_4 = df_pivot_4.dropna().reset_index(drop=True)\n",
    "\n",
    "df = df_pivot_4\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens30']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b703a111-9fe7-40e8-82a2-3e1f6628e2ed",
   "metadata": {},
   "source": [
    "#### Sensor 74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dcadb2a8-d382-4c8c-8601-8743df44c346",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b81e3158-777e-4fab-a3e1-caefa3fa0f8d",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "\n",
    "\n",
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8d73b558-d78b-41df-80d6-82356f92ed98",
   "metadata": {},
   "source": [
    "#### Sensor 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "507d165d-afcd-43f2-a7c3-65b724a48066",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05c7aff9-c3b3-4fd5-a41c-dd4d25286014",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "\n",
    "\n",
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6cebed0c-1324-496f-860f-dbad13da89c5",
   "metadata": {},
   "source": [
    "### Group 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb405e10-5608-4265-97a9-4f79de608313",
   "metadata": {},
   "source": [
    "df_group_5 = df_rovere[df_rovere['group_id'] == '5'].reset_index(drop=True)\n",
    "df_group_5 = df_group_5.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_5.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_5 = df_group_5.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_5.columns.name = None\n",
    "df_pivot_5.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "df_pivot_5 = df_pivot_5.dropna().reset_index(drop=True)\n",
    "\n",
    "df = df_pivot_5\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens60']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "45108717-77fa-4956-9017-4f9cd6cd3b0d",
   "metadata": {},
   "source": [
    "#### Sensor 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fdff8634-408d-49cf-8b80-bd3e291059ec",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6dc3b56f-3b73-4f17-bf86-69a64e4b6a22",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "\n",
    "\n",
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7ec635c4-8c5c-40ea-9f32-8e897b8ed1e5",
   "metadata": {},
   "source": [
    "#### Sensor 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d2a0482f-f4bb-4668-ab6a-074d28e5dfb7",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b656af9-30fd-48d4-a9df-7872d2e886c5",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "\n",
    "\n",
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b9bea4fa-6ff4-419d-bcc9-de67cfd6c6a3",
   "metadata": {},
   "source": [
    "### Group 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "128975d2-45a3-4322-b051-0fefd9c2bdcd",
   "metadata": {},
   "source": [
    "df_group_6 = df_rovere[df_rovere['group_id'] == '6'].reset_index(drop=True)\n",
    "df_group_6 = df_group_6.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_6.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_6 = df_group_6.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_6.columns.name = None\n",
    "df_pivot_6.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "df_pivot_6 = df_pivot_6.dropna().reset_index(drop=True)\n",
    "\n",
    "df = df_pivot_6\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens60']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9a914ce7-23f2-423f-a097-50b119a4c4fd",
   "metadata": {},
   "source": [
    "#### Sensor 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66f0fe75-74c3-4313-b8dc-402499f99dcc",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "900b1907-3f35-4364-be32-3ba36c960f9a",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "\n",
    "\n",
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "18988faf-c91a-405e-89c5-929a30d30472",
   "metadata": {},
   "source": [
    "#### Sensor 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a11c8f70-ef75-43fd-ab32-a7d68cef48c8",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7482201f-2d85-46e2-9c62-c48320add466",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "\n",
    "\n",
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e25f833e-f613-4707-b533-37fcedfc231a",
   "metadata": {},
   "source": [
    "### Group 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6109f79e-273d-46b8-b9da-fb80ba2df9ea",
   "metadata": {},
   "source": [
    "df_group_7 = df_rovere[df_rovere['group_id'] == '7'].reset_index(drop=True)\n",
    "df_group_7 = df_group_7.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_7.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_7 = df_group_7.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_7.columns.name = None\n",
    "df_pivot_7.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "df_pivot_7 = df_pivot_7.dropna().reset_index(drop=True)\n",
    "\n",
    "df = df_pivot_7\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens30']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "edf6239e-cc56-48c7-b09d-aef137f9d68c",
   "metadata": {},
   "source": [
    "#### Sensor 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b926261e-2c2d-41d4-b2dd-fcf1302a3954",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "730ad427-b8c0-4e5c-88cb-e9c70e5da9ec",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "\n",
    "\n",
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ea196faa-16dd-4495-a824-0c13bf6d072b",
   "metadata": {},
   "source": [
    "#### Sensor 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b2d8b41f-875b-4c27-bee7-c60e2cd4dc61",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "628e8abe-a2a6-41df-95f1-78aa8ed925b1",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "\n",
    "\n",
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9f34a422-95a7-4f83-b666-58ca2241232c",
   "metadata": {},
   "source": [
    "### Group 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "70181ee7-b242-421a-bda6-d6e5dde9b172",
   "metadata": {},
   "source": [
    "df_group_8 = df_rovere[df_rovere['group_id'] == '8'].reset_index(drop=True)\n",
    "df_group_8 = df_group_8.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_8.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_8 = df_group_8.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_8.columns.name = None\n",
    "df_pivot_8.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "df_pivot_8 = df_pivot_8.dropna().reset_index(drop=True)\n",
    "\n",
    "df = df_pivot_8\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens30']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "32ae5213-37c6-4fe4-8e40-06318f0992ab",
   "metadata": {},
   "source": [
    "#### Sensor 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "432d4ad8-45dd-420f-88d7-729a6da1c9f4",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4bf91770-cb89-43d1-b7b1-d37e56e77075",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "\n",
    "\n",
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5b016eef-512d-4700-9a1f-4a3262421ad2",
   "metadata": {},
   "source": [
    "#### Sensor 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c2626cbe-6657-4228-801e-eeab0f0260a9",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "all_features_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "all_features_predictions = all_features_model.predict(sm.add_constant(X_train))\n",
    "all_features_predictions_test = all_features_model.predict(sm.add_constant(X_test))\n",
    "\n",
    "selected_features = stepwise_bidirectional_selection(X_train, y_train, method='aic')\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train[selected_features])).fit()\n",
    "\n",
    "train_linear_pred = final_model.predict(sm.add_constant(X_train[selected_features]))\n",
    "linear_pred = final_model.predict(sm.add_constant(X_test[selected_features]))\n",
    "\n",
    "\n",
    "print(f'Number of Selected Features: {len(selected_features)}')\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "04489e37-d78a-4929-aace-d8635e4db733",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "X = X[selected_features]\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "result_df, order_arima = optimize_ARIMA(y_train, order_list, d)\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train[selected_features], order_list, d)\n",
    "\n",
    "\n",
    "model_arima = SARIMAX(endog=y_train, order=order_arima)\n",
    "results_arima = model_arima.fit(disp=False)\n",
    "\n",
    "res_train_arima = results_arima.resid\n",
    "train_arima_pred = y_train - res_train_arima\n",
    "\n",
    "\n",
    "model = SARIMAX(endog=y_train, exog=X_train[selected_features], order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arima_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    forecast = results_arima.get_prediction(start=len(y_new_train), end=len(y_new_train))\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arima_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, order=order_arima)\n",
    "    results_arima = model.fit(disp=False)\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train.copy()\n",
    "\n",
    "for i in range(len(X_test[selected_features])):\n",
    "    \n",
    "    next_exog = X_test.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear_all = round(mape(y_train, all_features_predictions), 3)\n",
    "mae_train_linear_all = round(mae(y_train, all_features_predictions), 3)\n",
    "rmse_train_linear_all = round(rmse(y_train, all_features_predictions), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arima = round(mape(y_train, train_arima_pred), 3)\n",
    "mae_train_arima = round(mae(y_train, train_arima_pred), 3)\n",
    "rmse_train_arima = round(rmse(y_train, train_arima_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear_all = round(mape(y_test, all_features_predictions_test), 3)\n",
    "mae_linear_all = round(mae(y_test, all_features_predictions_test), 3)\n",
    "rmse_linear_all = round(rmse(y_test, all_features_predictions_test), 3)\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_arima = round(mape(y_test, arima_pred), 3)\n",
    "mae_arima = round(mae(y_test, arima_pred), 3)\n",
    "rmse_arima = round(rmse(y_test, arima_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train ALL', 'LM Train', 'ARIMA Train', 'ARIMAX Train', 'LM Test ALL', 'LM Test', 'ARIMA Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear_all, mape_train_linear, mape_train_arima, mape_train_arimax, mape_linear_all, mape_linear, mape_arima, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear_all, mae_train_linear, mae_train_arima, mae_train_arimax, mae_linear_all, mae_linear, mae_arima, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear_all, rmse_train_linear, rmse_train_arima, rmse_train_arimax, rmse_linear_all, rmse_linear, rmse_arima, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
