{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e57ebabe-0025-43f1-906a-28914ca96007",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smts\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from typing import Union\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2de1762-41df-4388-a200-80e2e9c48d68",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "def fit_arimax_model(order, endog, exog, d):\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            model = SARIMAX(endog, exog=exog, order=(order[0], d, order[1]), simple_differencing=False).fit(disp=False)\n",
    "            aic = model.aic\n",
    "            bic = model.bic\n",
    "            return [(order[0], d, order[1]), round(aic, 2), round(bic, 2)]\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def optimize_ARIMAX(endog: Union[pd.Series, list], exog: Union[pd.Series, list], order_list: list, d: int) -> pd.DataFrame:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        results = Parallel(n_jobs=-1)(delayed(fit_arimax_model)(order, endog, exog, d) for order in tqdm(order_list))\n",
    "    \n",
    "    results = [result for result in results if result is not None]\n",
    "\n",
    "    result_df = pd.DataFrame(results, columns=['(p, d, q)', 'AIC', 'BIC'])\n",
    "    result_df = result_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\n",
    "    order_arimax = result_df['(p, d, q)'].iloc[0]    \n",
    "    \n",
    "    return result_df, order_arimax\n",
    "\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    non_zero_indices = np.where(y_true != 0)[0]\n",
    "    y_true_no_zeros = np.array(y_true)[non_zero_indices]\n",
    "    y_pred_no_zeros = np.array(y_pred)[non_zero_indices]\n",
    "\n",
    "    absolute_percentage_errors = np.abs((y_true_no_zeros - y_pred_no_zeros) / y_true_no_zeros)\n",
    "    mape_value = np.mean(absolute_percentage_errors) * 100\n",
    "    return mape_value\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b241a10-9641-414f-8d30-e331641d8d9b",
   "metadata": {},
   "source": [
    "df = pd.read_csv('data_sensors_rovere.csv')\n",
    "df = df.rename(columns={'group': 'group_id'})\n",
    "\n",
    "df_rovere = df[['reading_id', 'timestamp', 'sensor_id', 'value', 'description', 'group_id']]\n",
    "\n",
    "df_rovere['reading_id'] = df_rovere['reading_id'].astype(str)\n",
    "df_rovere['timestamp'] = pd.to_datetime(df_rovere['timestamp']).dt.floor('D').dt.date\n",
    "df_rovere['sensor_id'] = df_rovere['sensor_id'].astype(str)\n",
    "df_rovere['value'] = df_rovere['value'].astype(float)\n",
    "df_rovere['description'] = df_rovere['description'].astype(str)\n",
    "df_rovere['group_id'] = df_rovere['group_id'].astype(str)\n",
    "\n",
    "condition_30 = df_rovere['sensor_id'].isin(['72', '76', '73', '74', '61', '63', '67', '65'])\n",
    "condition_60 = df_rovere['sensor_id'].isin(['71', '69', '75', '70', '62', '64', '68', '66'])\n",
    "condition_irrigation = df_rovere['description'] == 'irrigation'\n",
    "\n",
    "df_rovere.loc[condition_30, 'description'] = 'Tensiometer 30'\n",
    "df_rovere.loc[condition_60, 'description'] = 'Tensiometer 60'\n",
    "df_rovere.loc[condition_irrigation, 'description'] = 'Irrigation'"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b4cf0b90-3d82-4745-b9be-951495a73f8b",
   "metadata": {},
   "source": [
    "### Group 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc16869-425b-4108-872b-24f561be0bb0",
   "metadata": {},
   "source": [
    "df_group_1 = df_rovere[df_rovere['group_id'] == '1'].reset_index(drop=True)\n",
    "df_group_1 = df_group_1.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_1.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_1 = df_group_1.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_1.columns.name = None\n",
    "df_pivot_1.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "\n",
    "df_pivot_1 = df_pivot_1.dropna().reset_index(drop=True)\n",
    "df = df_pivot_1\n",
    "\n",
    "\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens30']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "71aabb1c-7f66-4095-88dd-fa7e8dcd1fba",
   "metadata": {},
   "source": [
    "#### Sensor 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9fca92a-baf2-4762-a9ad-0234c13c4fb8",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6785570-3489-4045-8ea0-20c41b960fc6",
   "metadata": {},
   "source": [
    "pca = PCA(n_components=23)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c951873b-a81b-4291-aa14-6270ba5eeeb2",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e1e506-9313-4467-9ce3-4a17b0edc7ba",
   "metadata": {},
   "source": [
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d1db97d-f6dd-4565-af2f-c36a7f035842",
   "metadata": {},
   "source": [
    "residuals_naive = y_test - naive_pred[-len(y_test):]\n",
    "residuals_ARIMAX = y_test - arimax_pred\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "combined_index = np.concatenate([y_train.index, y_test.index])\n",
    "combined_index_1 = np.concatenate([y_train.index, y_test[:-1].index])\n",
    "\n",
    "new_indices = np.arange(len(y_train), len(y_train) + len(y_test))\n",
    "arimax_pred_df = pd.DataFrame(arimax_pred, index=new_indices, columns=['ARIMAX Prediction'])\n",
    "\n",
    "ax.plot(combined_index, np.concatenate([y_train, y_test]), label='Original Series', color='blue')\n",
    "ax.plot(y_test, 'b-', label='Actual', color='black')\n",
    "\n",
    "ax.plot(naive_pred[-len(y_test):], 'r:', label='Naive Method', color='red')\n",
    "ax.scatter(combined_index[-len(y_test):], residuals_naive, color='orange', label='Residuals Naive', marker='o')\n",
    "\n",
    "ax.plot(arimax_pred_df.iloc[:-1], 'k--', label='ARIMAX', color='green')\n",
    "ax.scatter(combined_index[-len(y_test):-1], residuals_ARIMAX[:-1], color='purple', label='Residuals ARIMAX', marker='x')\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Avg Tens 30')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d649acc6-b603-491a-a574-c251ad3e5462",
   "metadata": {},
   "source": [
    "#### Sensor 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afaf6b23-775d-4b0f-a4da-4f3e7c6ce40a",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8013e5e-6a62-40e9-a170-9e5af8aea2be",
   "metadata": {},
   "source": [
    "pca = PCA(n_components=23)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6e25e92-8a65-4e5e-9f24-c692617d2eb7",
   "metadata": {},
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "445ec007-54da-4b7a-ab63-4f4249d17a00",
   "metadata": {},
   "source": [
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a3ed70f-2b31-451b-9a70-2fc80749094d",
   "metadata": {},
   "source": [
    "residuals_naive = y_test - naive_pred[-len(y_test):]\n",
    "residuals_ARIMAX = y_test - arimax_pred\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "combined_index = np.concatenate([y_train.index, y_test.index])\n",
    "combined_index_1 = np.concatenate([y_train.index, y_test[:-1].index])\n",
    "\n",
    "new_indices = np.arange(len(y_train), len(y_train) + len(y_test))\n",
    "arimax_pred_df = pd.DataFrame(arimax_pred, index=new_indices, columns=['ARIMAX Prediction'])\n",
    "\n",
    "ax.plot(combined_index, np.concatenate([y_train, y_test]), label='Original Series', color='blue')\n",
    "ax.plot(y_test, 'b-', label='Actual', color='black')\n",
    "\n",
    "ax.plot(naive_pred[-len(y_test):], 'r:', label='Last Method', color='red')\n",
    "ax.scatter(combined_index[-len(y_test):], residuals_naive, color='orange', label='Residuals Naive', marker='o')\n",
    "\n",
    "ax.plot(arimax_pred_df.iloc[:-1], 'k--', label='ARIMAX', color='green')\n",
    "ax.scatter(combined_index[-len(y_test):-1], residuals_ARIMAX[:-1], color='purple', label='Residuals ARIMAX', marker='x')\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Avg Tens 60')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b9255905-b16b-4efa-96e8-3b9a11a9da52",
   "metadata": {},
   "source": [
    "### Group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc48321d-19b1-4928-bfc0-7bd513843c42",
   "metadata": {},
   "source": [
    "df_group_2 = df_rovere[df_rovere['group_id'] == '2'].reset_index(drop=True)\n",
    "df_group_2 = df_group_2.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_2.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_2 = df_group_2.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_2.columns.name = None\n",
    "df_pivot_2.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "\n",
    "df_pivot_2 = df_pivot_2.dropna().reset_index(drop=True)\n",
    "df = df_pivot_2\n",
    "\n",
    "\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens30']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1b41a9d6-2657-4a14-b0db-d46f5dfbbcc4",
   "metadata": {},
   "source": [
    "#### Sensor 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc004112-027e-4b2d-8b4b-b55b98abb287",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=24)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7b615c0-424e-4f6e-ac2f-59b097bbacdb",
   "metadata": {},
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "391903f6-f5f9-4e5a-b747-5eb7b6832713",
   "metadata": {},
   "source": [
    "#### Sensor 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7931fdd-24a8-426f-bacb-b40ce4f1f1b3",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=23)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdb7963e-6609-4ada-ba85-3460e3bad428",
   "metadata": {},
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ebd59ccf-a142-463e-9fc7-05dea26bea7f",
   "metadata": {},
   "source": [
    "### Group 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d9b9c6b-281d-4355-ba23-237e082dfc4d",
   "metadata": {},
   "source": [
    "df_group_3 = df_rovere[df_rovere['group_id'] == '3'].reset_index(drop=True)\n",
    "df_group_3 = df_group_3.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_3.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_3 = df_group_3.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_3.columns.name = None\n",
    "df_pivot_3.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "\n",
    "df_pivot_3 = df_pivot_3.dropna().reset_index(drop=True)\n",
    "df = df_pivot_3\n",
    "\n",
    "\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens30']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "82571a76-8822-4627-9c49-5c156521dd8c",
   "metadata": {},
   "source": [
    "#### Sensor 73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afe7dfd9-879f-4a66-bd33-9a21a855280c",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=24)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "482f1736-a835-4651-8ff0-392ebb5676d9",
   "metadata": {},
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dc7d2512-5aaa-47f9-a47c-6e1db9b1595f",
   "metadata": {},
   "source": [
    "#### Sensor 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8394e51a-d07a-4690-b02c-ec1dacaf1df5",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=24)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9914b42-4c90-4784-97d8-11b8691a15c5",
   "metadata": {},
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "254f7af5-b38b-407c-adba-b6124c85785f",
   "metadata": {},
   "source": [
    "### Group 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a32549c-48b8-4702-abd3-e86da5ff02d5",
   "metadata": {},
   "source": [
    "df_group_4 = df_rovere[df_rovere['group_id'] == '4'].reset_index(drop=True)\n",
    "df_group_4 = df_group_4.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_4.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_4 = df_group_4.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_4.columns.name = None\n",
    "df_pivot_4.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "\n",
    "df_pivot_4 = df_pivot_4.dropna().reset_index(drop=True)\n",
    "df = df_pivot_4\n",
    "\n",
    "\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens30']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9aec5f19-5ea2-46fc-ae62-ae88dd3fbde0",
   "metadata": {},
   "source": [
    "#### Sensor 74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "233a370e-5b87-40f8-a20d-711549189f14",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=23)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09e7c0e9-38db-4f00-abbc-e452a0b99e84",
   "metadata": {},
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "321f9b71-d042-4fa9-acf9-bc357536814b",
   "metadata": {},
   "source": [
    "#### Sensor 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbb32bbb-946f-4ed4-96d2-10ede4cb0f97",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=24)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "798e0582-8d45-4fff-94bd-047db2986f2a",
   "metadata": {},
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3b411d91-1fc2-404e-97a8-4eedf4305db0",
   "metadata": {},
   "source": [
    "### Group 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2103a46-2ef6-4ecc-985d-60ad5926be32",
   "metadata": {},
   "source": [
    "df_group_5 = df_rovere[df_rovere['group_id'] == '5'].reset_index(drop=True)\n",
    "df_group_5 = df_group_5.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_5.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_5 = df_group_5.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_5.columns.name = None\n",
    "df_pivot_5.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "df_pivot_5 = df_pivot_5.dropna().reset_index(drop=True)\n",
    "\n",
    "df = df_pivot_5\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens60']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4f7720b9-5be7-4167-8a15-47304e66b5cc",
   "metadata": {},
   "source": [
    "#### Sensor 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba6281ef-8438-4aa3-b9d3-5f634b6cede0",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=23)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ac48744-8085-4d6c-860b-d7930c09cb5a",
   "metadata": {},
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4b3e4da9-6300-4f3c-abe8-12a93873ecca",
   "metadata": {},
   "source": [
    "#### Sensor 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b518d26c-8183-4ec3-91ea-223306edda21",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=24)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7c15d7a-1992-4ea7-bc14-11a80427b801",
   "metadata": {},
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ac7a3504-708a-4c56-bb5a-9ae29fd114c2",
   "metadata": {},
   "source": [
    "### Group 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5c3867c-e7a9-4f3d-9d3a-bdf72d3edad0",
   "metadata": {},
   "source": [
    "df_group_6 = df_rovere[df_rovere['group_id'] == '6'].reset_index(drop=True)\n",
    "df_group_6 = df_group_6.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_6.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_6 = df_group_6.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_6.columns.name = None\n",
    "df_pivot_6.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "df_pivot_6 = df_pivot_6.dropna().reset_index(drop=True)\n",
    "\n",
    "df = df_pivot_6\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens60']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "98ff3298-1ac7-4532-91bf-01b427e0f578",
   "metadata": {},
   "source": [
    "#### Sensor 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3dbec83-a8f4-46ca-958d-7c3b8cf61bb8",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=24)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d197cbe1-a2aa-4100-b144-b1b53233a940",
   "metadata": {},
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "afad6d14-2e51-4103-93c1-8b429a33b3f6",
   "metadata": {},
   "source": [
    "#### Sensor 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8808b645-bf4b-4251-ab73-4cbc7724b783",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=23)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7fc35ae4-11f8-40c3-b2e7-ed894cee7fde",
   "metadata": {},
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6f0d73eb-c9c3-4ecc-9564-b5fe93a971bf",
   "metadata": {},
   "source": [
    "### Group 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb6b549e-568f-4b30-81c3-f0ccbb6c499a",
   "metadata": {},
   "source": [
    "df_group_7 = df_rovere[df_rovere['group_id'] == '7'].reset_index(drop=True)\n",
    "df_group_7 = df_group_7.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_7.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_7 = df_group_7.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_7.columns.name = None\n",
    "df_pivot_7.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "df_pivot_7 = df_pivot_7.dropna().reset_index(drop=True)\n",
    "\n",
    "df = df_pivot_7\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens30']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dc3efaab-ff61-48d1-96da-9e67262dbf94",
   "metadata": {},
   "source": [
    "#### Sensor 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ad5792e-8b98-44a8-b185-b2870f364d37",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=24)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "146e1cc5-4949-4f22-9197-a0cd6431aff9",
   "metadata": {},
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f409ee37-dbfc-40e2-9f9d-96c34df7ca94",
   "metadata": {},
   "source": [
    "#### Sensor 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4031b153-24ea-4afa-a570-e46c6c725790",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=24)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4eab212b-497e-436c-a2a7-b7962ecc009d",
   "metadata": {},
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3a1fb0e5-5760-450b-b38f-61fb39b54539",
   "metadata": {},
   "source": [
    "### Group 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf80a340-fd36-4b28-a6b3-fb302dbcb2f5",
   "metadata": {},
   "source": [
    "df_group_8 = df_rovere[df_rovere['group_id'] == '8'].reset_index(drop=True)\n",
    "df_group_8 = df_group_8.groupby(['timestamp', 'description']).agg({'value': ['min', 'max', 'mean', 'median', 'sum']}).reset_index()\n",
    "df_group_8.columns = ['timestamp', 'description', 'val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']\n",
    "\n",
    "df_pivot_8 = df_group_8.pivot(index='timestamp', columns='description', values=['val_min', 'val_max', 'val_avg', 'val_med', 'val_sum']).reset_index()\n",
    "df_pivot_8.columns.name = None\n",
    "df_pivot_8.columns = ['date', 'min_hum', 'min_temp', 'min_solar', 'min_irr', 'min_rain', 'min_tens30', 'min_tens60',\n",
    "                      'max_hum', 'max_temp', 'max_solar', 'max_irr', 'max_rain', 'max_tens30', 'max_tens60',\n",
    "                      'avg_hum', 'avg_temp', 'avg_solar', 'avg_irr', 'avg_rain', 'avg_tens30', 'avg_tens60',\n",
    "                      'med_hum', 'med_temp', 'med_solar', 'med_irr', 'med_rain', 'med_tens30', 'med_tens60',\n",
    "                      'sum_hum', 'sum_temp', 'sum_solar', 'sum_irr', 'sum_rain', 'sum_tens30', 'sum_tens60']\n",
    "df_pivot_8 = df_pivot_8.dropna().reset_index(drop=True)\n",
    "\n",
    "df = df_pivot_8\n",
    "columns_to_drop = ['min_irr', 'max_irr', 'avg_irr', 'med_irr', 'min_rain', 'sum_hum', 'sum_temp', 'sum_solar', 'avg_rain']\n",
    "df = df.drop(columns=columns_to_drop).dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "col_drop_30 = ['min_tens60', 'max_tens60', 'avg_tens60', 'med_tens60', 'sum_tens60']\n",
    "df_30 = df.drop(columns=col_drop_30).dropna().reset_index(drop=True)\n",
    "\n",
    "col_drop_60 = ['min_tens30', 'max_tens30', 'avg_tens30', 'med_tens30', 'sum_tens30']\n",
    "df_60 = df.drop(columns=col_drop_60).dropna().reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bd736307-eebb-4ed9-9c82-23516418d217",
   "metadata": {},
   "source": [
    "#### Sensor 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9684dce-74ce-4af0-8942-afeb7eb69d61",
   "metadata": {},
   "source": [
    "y = df_30['avg_tens30']\n",
    "X = df_30.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens30_lag1', 'avg_tens30_lag2', 'avg_tens30_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=23)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f418ba2-b077-4588-9d01-2d14f8f4b5ef",
   "metadata": {},
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "91972510-81c2-4875-a1d4-c6c832f2cdd7",
   "metadata": {},
   "source": [
    "#### Sensor 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "deb97b4c-1340-4113-8fa6-b9bec9388cb1",
   "metadata": {},
   "source": [
    "y = df_60['avg_tens60']\n",
    "X = df_60.drop(['date'], axis=1)\n",
    "\n",
    "X = X.shift(1).add_suffix('_lag1').join(X.shift(2).add_suffix('_lag2')).join(X.shift(3).add_suffix('_lag3'))\n",
    "\n",
    "columns_to_drop = ['avg_tens60_lag1', 'avg_tens60_lag2', 'avg_tens60_lag3']\n",
    "X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "y = y.iloc[3:].reset_index(drop=True)\n",
    "X = X.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.29878, shuffle=False)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=24)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_train_pca = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "X_test_pca = pd.DataFrame(X_test_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24'])\n",
    "\n",
    "explained_variances = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = explained_variances.cumsum()\n",
    "\n",
    "for i, cumulative_variance in enumerate(cumulative_explained_variance):\n",
    "    print(f'Cumulative Variance Explained by first {i + 1} PCs: {cumulative_variance:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb008103-a1f6-4466-b1d6-e1a21a2200b9",
   "metadata": {},
   "source": [
    "naive_pred = pd.concat([y_train.shift(1).fillna(y_train.iloc[-1])[1:], y_test.shift(1).fillna(y_train.iloc[-1])])\n",
    "\n",
    "linear_reg_model = LinearRegression()\n",
    "linear_reg_model.fit(X_train_pca, y_train)\n",
    "\n",
    "train_linear_pred = linear_reg_model.predict(X_train_pca)\n",
    "linear_pred = linear_reg_model.predict(X_test_pca)\n",
    "\n",
    "\n",
    "ps = range(0, 11, 1)\n",
    "qs = range(0, 11, 1)\n",
    "d = 1\n",
    "\n",
    "order_list = list(product(ps, qs))\n",
    "\n",
    "\n",
    "result_df, order_arimax = optimize_ARIMAX(y_train, X_train_pca, order_list, d)\n",
    "model = SARIMAX(endog=y_train, exog=X_train_pca, order=order_arimax)\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "res_train_arimax = results.resid\n",
    "train_arimax_pred = y_train - res_train_arimax\n",
    "\n",
    "\n",
    "arimax_pred = []\n",
    "y_new_train = y_train.copy()\n",
    "X_new_train = X_train_pca.copy()\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    \n",
    "    next_exog = X_test_pca.iloc[i, :]\n",
    "    \n",
    "    forecast = results.get_forecast(steps=1, exog=next_exog)\n",
    "    forecast_value = forecast.predicted_mean.values[0]\n",
    "    arimax_pred.append(forecast_value)\n",
    "\n",
    "    y_new_train = pd.concat([y_new_train, pd.Series([y_test.iloc[i]], index=[y_new_train.index[-1] + 1])])\n",
    "    X_new_train = pd.concat([X_new_train, next_exog.to_frame().transpose()], ignore_index=True)\n",
    "    \n",
    "    model = SARIMAX(endog=y_new_train, exog=X_new_train, order=order_arimax)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "\n",
    "mape_naive = round(mape(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "mae_naive = round(mae(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "rmse_naive = round(rmse(pd.concat([y_train[1:], y_test]), naive_pred), 3)\n",
    "\n",
    "mape_train_linear = round(mape(y_train, train_linear_pred), 3)\n",
    "mae_train_linear = round(mae(y_train, train_linear_pred), 3)\n",
    "rmse_train_linear = round(rmse(y_train, train_linear_pred), 3)\n",
    "\n",
    "mape_train_arimax = round(mape(y_train, train_arimax_pred), 3)\n",
    "mae_train_arimax = round(mae(y_train, train_arimax_pred), 3)\n",
    "rmse_train_arimax = round(rmse(y_train, train_arimax_pred), 3)\n",
    "\n",
    "\n",
    "mape_linear = round(mape(y_test, linear_pred), 3)\n",
    "mae_linear = round(mae(y_test, linear_pred), 3)\n",
    "rmse_linear = round(rmse(y_test, linear_pred), 3)\n",
    "\n",
    "mape_ARIMAX = round(mape(y_test, arimax_pred), 3)\n",
    "mae_ARIMAX = round(mae(y_test, arimax_pred), 3)\n",
    "rmse_ARIMAX = round(rmse(y_test, arimax_pred), 3)\n",
    "\n",
    "\n",
    "table = [\n",
    "    ['Metric', 'Naive Method', 'LM Train', 'ARIMAX Train', 'LM Test', 'ARIMAX Test'],\n",
    "    ['MAPE', mape_naive, mape_train_linear, mape_train_arimax, mape_linear, mape_ARIMAX],\n",
    "    ['MAE', mae_naive, mae_train_linear, mae_train_arimax, mae_linear, mae_ARIMAX],\n",
    "    ['RMSE', rmse_naive, rmse_train_linear, rmse_train_arimax, rmse_linear, rmse_ARIMAX]\n",
    "]\n",
    "\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
